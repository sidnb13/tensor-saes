defaults:
  - _self_

model: gpt2
dataset: togethercomputer/RedPajama-Data-1T-Sample
split: train
ctx_len: 64
hf_token: null
load_in_8bit: false
max_examples: -1
data_preprocessing_num_proc: 48

# distributed
ddp: false
tp: false

sae:
  expansion_factor: 32
  normalize_decoder: true
  scale_encoder_k: false
  scale_encoder_fvu: null
  num_latents: 0
  k: 32
  signed: false

batch_size: 8
grad_acc_steps: 1
micro_acc_steps: 1
lr: null
lr_warmup_steps: 1000
auxk_alpha: 0.0
dead_feature_threshold: 10000000
enable_cross_layer_training: false
hookpoints: []
layers: []
layer_stride: 1
distribute_modules: false
save_every: 1000
root_path: checkpoints
log_to_wandb: true
run_name: null
wandb_log_frequency: 1
