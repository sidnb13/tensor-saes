{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "# testing consequences of interventions on gpt2, and how they match up against our SAE's\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from typing import Dict, List\n",
    "\n",
    "import datasets\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum\n",
    "from safetensors.torch import load_file\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from sae.data import chunk_and_tokenize\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/home/sid/tensor-sae/checkpoints/pythia14m-all-layers-rp1t/pythia70m-all-layers-rp1t-sample_20240912_003009/layers.0_layers.1_layers.2_layers.3_layers.4_layers.5/sae-915.safetensors\"\n",
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use jacrevd need eager implementation\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, attn_implementation=\"eager\"\n",
    ").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sae_ckpt = load_file(ckpt_path, device=\"cuda:0\")\n",
    "\n",
    "feature_encoder_weights = sae_ckpt.get(\"encoder.weight\", sae_ckpt.get(\"weight\"))\n",
    "feature_encoder_bias = sae_ckpt.get(\"encoder.bias\", sae_ckpt.get(\"bias\"))\n",
    "# legacy keys\n",
    "feature_decoder_weights = sae_ckpt[\"decoder.weight\"]\n",
    "feature_decoder_bias = sae_ckpt[\"decoder.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "seq_len = 64\n",
    "num_samples = 100\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "dataset = dataset.train_test_split(\n",
    "    test_size=0.8, seed=seed\n",
    ").get(\"test\").select(range(num_samples))\n",
    "\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, max_seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24576])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_encoder_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InterventionOutputs:\n",
    "    activation_positions: torch.Tensor\n",
    "    causal_embeddings: torch.Tensor\n",
    "    v_j: torch.Tensor\n",
    "    v_k: torch.Tensor\n",
    "    is_valid: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FeatureStats:\n",
    "    causality: List[float] = field(default_factory=list)\n",
    "    cosine: List[float] = field(default_factory=list)\n",
    "    error: List[float] = field(default_factory=list)\n",
    "    feature_activation_strength: List[float] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GlobalFeatureStatistics:\n",
    "    feature_activation_rate: torch.Tensor\n",
    "    total_active_features: float\n",
    "    avg_active_features_per_token: float\n",
    "    feature_dict: Dict[int, FeatureStats]\n",
    "\n",
    "\n",
    "def compute_feature_statistics(\n",
    "    model,\n",
    "    tokenized,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    sae_top_k: int = 128,\n",
    "    batch_size: int = 256,\n",
    "    exclude_first_k_tokens: int = 4,\n",
    "):\n",
    "    # (N,)\n",
    "    total_active_features = torch.zeros(\n",
    "        feature_encoder_weights.shape[0], device=model.device\n",
    "    )\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        tokenized, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = torch.ones_like(input_ids, device=model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "            hiddens = outputs.hidden_states\n",
    "\n",
    "        stacked_hiddens = torch.cat(hiddens[1:], dim=-1)[:, exclude_first_k_tokens:, :]\n",
    "\n",
    "        encoded_features = torch.einsum(\n",
    "            \"be,nse->nsb\", feature_encoder_weights, stacked_hiddens\n",
    "        )\n",
    "        encoded_features = encoded_features + feature_encoder_bias.unsqueeze(\n",
    "            0\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        k_th_strongest = (\n",
    "            torch.topk(encoded_features, k=sae_top_k, dim=-1)\n",
    "            .values[:, :, -1]\n",
    "            .unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        batch_binary_mask = (encoded_features >= k_th_strongest).float()\n",
    "\n",
    "        total_active_features += batch_binary_mask.sum(dim=(0, 1))\n",
    "\n",
    "    seq_len = tokenized[0][\"input_ids\"].shape[0] - exclude_first_k_tokens\n",
    "    feature_activation_rate = total_active_features / (len(tokenized) * seq_len)\n",
    "\n",
    "    total_active = total_active_features.sum().item()\n",
    "    avg_active_per_token = total_active / (len(tokenized) * seq_len)\n",
    "\n",
    "    feature_dict = {i: FeatureStats() for i in range(feature_encoder_weights.shape[-1])}\n",
    "\n",
    "    return GlobalFeatureStatistics(\n",
    "        feature_activation_rate=feature_activation_rate,\n",
    "        total_active_features=total_active,\n",
    "        avg_active_features_per_token=avg_active_per_token,\n",
    "        feature_dict=feature_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_intervention(\n",
    "    model: torch.nn.Module,\n",
    "    batch: torch.Tensor,\n",
    "    feature_activation_rate: torch.Tensor,\n",
    "    intervention_index: int,\n",
    "    readout_index: int,\n",
    "    feature_encoder_weights: torch.Tensor,\n",
    "    feature_encoder_bias: torch.Tensor,\n",
    "    feature_decoder_weights: torch.Tensor,\n",
    "    lambda_value: float = 1.0,\n",
    "    num_tokens: int = 1,\n",
    "    feature_top_k: int = None,\n",
    "    sae_top_k: int = 128,\n",
    ") -> InterventionOutputs:\n",
    "    \"\"\"\n",
    "    Perform an intervention on a model's activations using Sparse Autoencoder (SAE) features.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to intervene on.\n",
    "        batch: Input tensor to the model.\n",
    "        feature_activation_rate: the global feature activation rate statistic\n",
    "        intervention_index: Index of the layer to intervene on.\n",
    "        readout_index: Index of the layer to read out from.\n",
    "        feature_encoder_weights: Weights of the SAE encoder.\n",
    "        feature_encoder_bias: Bias of the SAE encoder.\n",
    "        feature_decoder_weights: Weights of the SAE decoder.\n",
    "        lambda_value: Strength of the intervention (default: 1.0).\n",
    "        num_tokens: Number of tokens to intervene on (default: 1).\n",
    "        feature_top_k: Index of the specific feature to intervene on.\n",
    "        sae_top_k: Number of top SAE features to consider.\n",
    "\n",
    "    Returns:\n",
    "        the results of the intervention as an InterventionOutputs object\n",
    "    \"\"\"\n",
    "\n",
    "    activation_positions = None\n",
    "    causal_embeddings = None\n",
    "    # j < k in layer idx\n",
    "    v_j = None\n",
    "    v_k = None\n",
    "    is_valid = None\n",
    "\n",
    "    global_top_feature_indices = torch.topk(\n",
    "        feature_activation_rate, k=sae_top_k, dim=-1\n",
    "    ).indices\n",
    "\n",
    "    def strengthen_specific_features(module, input, output, layer_offset=0):\n",
    "        nonlocal activation_positions, causal_embeddings, v_j, v_k, is_valid\n",
    "\n",
    "        embed_dim = output[0].shape[-1]\n",
    "        feature_encoder_segment = feature_encoder_weights[\n",
    "            :,\n",
    "            (intervention_index - layer_offset) * embed_dim : (\n",
    "                intervention_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "        feature_decoder_segment = feature_decoder_weights[\n",
    "            :,\n",
    "            (intervention_index - layer_offset) * embed_dim : (\n",
    "                intervention_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "\n",
    "        # Encode input activations\n",
    "        feature_activation = (\n",
    "            einsum(output[0], feature_encoder_segment.T, \"b s e, e n -> b s n\")\n",
    "            - feature_encoder_bias\n",
    "        )\n",
    "\n",
    "        batch_size, seq_len, _ = output[0].shape\n",
    "        causal_embeddings = output[0]\n",
    "\n",
    "        # Create a mask for active features\n",
    "        feature_mask = torch.zeros(\n",
    "            batch_size,\n",
    "            seq_len,\n",
    "            feature_activation.shape[-1],\n",
    "            device=output[0].device,\n",
    "            dtype=torch.bool,\n",
    "        )\n",
    "        feature_mask.scatter_(-1, global_top_feature_indices[None, None, :], 1)\n",
    "        feature_activation_global_masked = feature_activation * feature_mask\n",
    "\n",
    "        # Compute top-k index based on global activation\n",
    "        _, top_k_feature_index = torch.kthvalue(\n",
    "            feature_activation_global_masked,\n",
    "            k=feature_top_k,\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        # Get the decoder vectors for the specified feature index\n",
    "        v_j = feature_decoder_segment[top_k_feature_index]\n",
    "\n",
    "        # Select the tokens with the highest activation of the pinned feature\n",
    "        _, activation_positions = torch.topk(\n",
    "            feature_activation_global_masked, k=num_tokens, dim=-1\n",
    "        )\n",
    "\n",
    "        # Create a mask for the selected token positions\n",
    "        token_mask = torch.zeros(\n",
    "            batch_size,\n",
    "            seq_len,\n",
    "            feature_activation_global_masked.shape[-1],\n",
    "            device=output[0].device,\n",
    "        )\n",
    "        token_mask.scatter_(-1, activation_positions, 1)\n",
    "\n",
    "        new_output = output[0].clone()\n",
    "\n",
    "        # Add lambda * v_j only to the selected token positions\n",
    "        new_output += lambda_value * v_j * token_mask.any(-1, keepdim=True)\n",
    "        new_outputs = [new_output] + list(output[1:])\n",
    "\n",
    "        # Assign v_k\n",
    "        intervention_decoder_segment = feature_decoder_weights[\n",
    "            :,\n",
    "            (readout_index - layer_offset) * embed_dim : (\n",
    "                readout_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "        v_k = intervention_decoder_segment[top_k_feature_index]\n",
    "\n",
    "        # Check if the feature fires in any of the tokens\n",
    "        is_valid = (\n",
    "            (feature_activation_global_masked[:, :, top_k_feature_index] > 0)\n",
    "            .any(dim=1)\n",
    "            .bool()\n",
    "        )\n",
    "\n",
    "        return tuple(new_outputs)\n",
    "\n",
    "    if \"gpt\" in model_name:\n",
    "        intervention_hook = model.transformer.h[\n",
    "            intervention_index\n",
    "        ].register_forward_hook(\n",
    "            partial(strengthen_specific_features, layer_offset=intervention_index)\n",
    "        )\n",
    "    else:\n",
    "        intervention_hook = model.gpt_neox.layers[\n",
    "            intervention_index\n",
    "        ].register_forward_hook(\n",
    "            partial(strengthen_specific_features, layer_offset=intervention_index)\n",
    "        )\n",
    "\n",
    "    # do interventions by running hooks\n",
    "    with intervention_hook, torch.no_grad():\n",
    "        model(**batch)\n",
    "\n",
    "    return InterventionOutputs(\n",
    "        activation_positions, causal_embeddings, v_j, v_k, is_valid\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 7/7 [00:01<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of active features: 13301770.0\n",
      "Average number of active features per token: 128.00\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "stats = compute_feature_statistics(\n",
    "    model,\n",
    "    tokenized,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    sae_top_k=128\n",
    ")\n",
    "\n",
    "print(f\"Total number of active features: {stats.total_active_features}\")\n",
    "print(f\"Average number of active features per token: {stats.avg_active_features_per_token:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# Example usage with specific feature indices\n",
    "intervention_index = 4\n",
    "readout_index = 5\n",
    "text = [\"Hello, world!\", \"Hello, world!\"]\n",
    "test_batch = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "feature_top_k = 10\n",
    "\n",
    "intervention = perform_intervention(\n",
    "    model=model,\n",
    "    batch=test_batch,\n",
    "    feature_activation_rate=stats.feature_activation_rate,\n",
    "    intervention_index=intervention_index,\n",
    "    readout_index=readout_index,\n",
    "    feature_encoder_weights=feature_encoder_weights,\n",
    "    feature_encoder_bias=feature_encoder_bias,\n",
    "    feature_decoder_weights=feature_decoder_weights,\n",
    "    lambda_value=1.0,\n",
    "    num_tokens=1,\n",
    "    feature_top_k=feature_top_k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[21350],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    1]],\n",
       "\n",
       "        [[    0],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    1]]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervention.activation_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
