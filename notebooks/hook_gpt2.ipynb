{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing consequences of interventions on gpt2, and how they match up against our SAE's\n",
    "from functools import partial\n",
    "\n",
    "import datasets\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum\n",
    "from safetensors.torch import load_file\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from sae.data import chunk_and_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "ckpt_path = \"/home/sid/tensor-sae/checkpoints/all-layer-test/sae.safetensors\"\n",
    "# ckpt_path = \"/home/sid/tensor-sae/checkpoints/pythia14m-all-layers-rp1t/pythia14m-all-layers-rp1t-sample_20240901_123737/layers.0_layers.1_layers.2_layers.3_layers.4_layers.5/sae-2298.safetensors\"\n",
    "# model_name = \"EleutherAI/pythia-14m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output embeddings: torch.Size([2, 768])\n",
      "shape of causal embeddings: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# to use jacrevd need eager implementation\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, attn_implementation=\"eager\"\n",
    ").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sae_ckpt = load_file(ckpt_path, device=\"cuda\")\n",
    "\n",
    "feature_encoder_weights = sae_ckpt.get(\"encoder.weight\", sae_ckpt.get(\"weight\"))\n",
    "feature_encoder_bias = sae_ckpt.get(\"encoder.bias\", sae_ckpt.get(\"bias\"))\n",
    "# legacy keys\n",
    "feature_decoder_weights = sae_ckpt[\"decoder.weight\"]\n",
    "feature_decoder_bias = sae_ckpt[\"decoder.bias\"]\n",
    "\n",
    "intervention_index = 2\n",
    "readout_index = 4\n",
    "\n",
    "\n",
    "def create_hooks(\n",
    "    model,\n",
    "    intervention_index,\n",
    "    readout_index,\n",
    "    lambda_value,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    feature_select_k=1,  # take top k-th feature\n",
    "    num_tokens=1,\n",
    "):\n",
    "    activation_positions = None\n",
    "    consequent_embeddings = None\n",
    "    causal_embeddings = None\n",
    "    # j < k in layer idx\n",
    "    v_j = None\n",
    "    v_k = None\n",
    "\n",
    "    def strengthen_sae_feature(module, input, output, layer_offset=0):\n",
    "        nonlocal activation_positions, causal_embeddings, v_j, v_k\n",
    "\n",
    "        embed_dim = output[0].shape[-1]\n",
    "        feature_encoder_segment = feature_encoder_weights[\n",
    "            :,\n",
    "            (intervention_index - layer_offset) * embed_dim : (\n",
    "                intervention_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "        feature_decoder_segment = feature_decoder_weights[\n",
    "            :,\n",
    "            (intervention_index - layer_offset) * embed_dim : (\n",
    "                intervention_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "\n",
    "        feature_activation = (\n",
    "            einsum(output[0], feature_encoder_segment.T, \"b s e, e n -> b s n\")\n",
    "            - feature_encoder_bias\n",
    "        )\n",
    "        # shape (batch_size, seq_len, 1)\n",
    "        feature_activation, max_feature_index = torch.kthvalue(\n",
    "            feature_activation,\n",
    "            k=feature_activation.shape[-1] - feature_select_k,\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        activation_positions = (\n",
    "            (feature_activation > 0).float().topk(k=num_tokens, dim=1)[1]\n",
    "        )\n",
    "        has_activation = (feature_activation > 0).any(dim=1)\n",
    "        activation_positions[~has_activation] = -1\n",
    "\n",
    "        batch_size, seq_len, embed_dim = output[0].shape\n",
    "\n",
    "        mask = (\n",
    "            torch.arange(seq_len, device=output[0].device)[None, :].expand(\n",
    "                batch_size, -1\n",
    "            )\n",
    "            == activation_positions\n",
    "        )\n",
    "\n",
    "        causal_embeddings = output[0]\n",
    "\n",
    "        # (batch_size, seq_len, embed_dim)\n",
    "        v_j = (\n",
    "            feature_decoder_segment.unsqueeze(0)\n",
    "            .expand(output[0].shape[0], -1, -1)\n",
    "            .gather(1, max_feature_index.unsqueeze(-1).expand(-1, -1, embed_dim))\n",
    "        )\n",
    "\n",
    "        new_output = output[0] + lambda_value * mask[:, :, None] * v_j\n",
    "\n",
    "        intervention_decoder_segment = feature_decoder_weights[\n",
    "            :,\n",
    "            (readout_index - layer_offset) * embed_dim : (\n",
    "                readout_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "        v_k = (\n",
    "            intervention_decoder_segment.unsqueeze(0)\n",
    "            .expand(output[0].shape[0], -1, -1)\n",
    "            .gather(1, max_feature_index.unsqueeze(-1).expand(-1, -1, embed_dim))\n",
    "        )\n",
    "\n",
    "        new_outputs = [new_output] + list(output[1:])\n",
    "        return tuple(new_outputs)\n",
    "\n",
    "    def return_consequent_layer(module, input, output):\n",
    "        nonlocal consequent_embeddings, activation_positions\n",
    "\n",
    "        filtered_output = output[0]\n",
    "        # TODO: best to sum over the tokens? One way of reducing bias\n",
    "        consequent_embeddings = filtered_output.sum(dim=1)\n",
    "\n",
    "        # Return the original output unchanged\n",
    "        return output\n",
    "\n",
    "    if \"gpt\" in model_name:\n",
    "        intervention_hook = model.transformer.h[\n",
    "            intervention_index\n",
    "        ].register_forward_hook(\n",
    "            partial(strengthen_sae_feature, layer_offset=intervention_index)\n",
    "        )\n",
    "        readout_hook = model.transformer.h[readout_index].register_forward_hook(\n",
    "            return_consequent_layer\n",
    "        )\n",
    "    else:\n",
    "        intervention_hook = model.gpt_neox.layers[\n",
    "            intervention_index\n",
    "        ].register_forward_hook(\n",
    "            partial(strengthen_sae_feature, layer_offset=intervention_index)\n",
    "        )\n",
    "        readout_hook = model.gpt_neox.layers[readout_index].register_forward_hook(\n",
    "            return_consequent_layer\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        intervention_hook,\n",
    "        readout_hook,\n",
    "        lambda: activation_positions,\n",
    "        lambda: consequent_embeddings,\n",
    "        lambda: causal_embeddings,\n",
    "        lambda: v_j,\n",
    "        lambda: v_k,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_text(\n",
    "    model,\n",
    "    inputs,\n",
    "    intervention_index,\n",
    "    readout_index,\n",
    "    lam,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    feature_select_k,\n",
    "    num_tokens,\n",
    "):\n",
    "    (\n",
    "        intervention_hook,\n",
    "        readout_hook,\n",
    "        get_first_activation_positions,\n",
    "        get_consequent_embeddings,\n",
    "        get_causal_embeddings,\n",
    "        get_v_j,\n",
    "        get_v_k,\n",
    "    ) = create_hooks(\n",
    "        model,\n",
    "        intervention_index,\n",
    "        readout_index,\n",
    "        lam,\n",
    "        feature_encoder_weights,\n",
    "        feature_encoder_bias,\n",
    "        feature_decoder_weights,\n",
    "        feature_select_k,\n",
    "        num_tokens,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "    first_activation_positions = get_first_activation_positions()\n",
    "    consequent_embeddings = get_consequent_embeddings()\n",
    "    causal_embeddings = get_causal_embeddings()\n",
    "    v_j = get_v_j()\n",
    "    v_k = get_v_k()\n",
    "\n",
    "    intervention_hook.remove()\n",
    "    readout_hook.remove()\n",
    "\n",
    "    return (\n",
    "        first_activation_positions,\n",
    "        consequent_embeddings,\n",
    "        causal_embeddings,\n",
    "        v_j,\n",
    "        v_k,\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "intervention_index = 4\n",
    "readout_index = 5\n",
    "text = [\"Hello, world!\", \"Hello, world!\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Assuming you have these variables defined\n",
    "# feature_encoder_weights, feature_encoder_bias, feature_decoder_weights\n",
    "\n",
    "first_activation_positions, consequent_embeddings, causal_embeddings, _, _ = (\n",
    "    process_text(\n",
    "        model,\n",
    "        inputs,\n",
    "        intervention_index,\n",
    "        readout_index,\n",
    "        1.0,\n",
    "        feature_encoder_weights,\n",
    "        feature_encoder_bias,\n",
    "        feature_decoder_weights,\n",
    "        1,\n",
    "        1,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"shape of output embeddings: {consequent_embeddings.shape}\")\n",
    "print(f\"shape of causal embeddings: {causal_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next code we would need to add:\n",
    "#filter out any batch elements where the SAE doesn't trigger\n",
    "#compare to activations from a hook on the clean sequence\n",
    "#subtract, compute comparisons, etc.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I suspect the most efficient way to go about thos jacobian computation is to modify the gpt2 forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 16. Reducing num_proc to 16 for dataset of size 16.\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ").select(range(16))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, max_seq_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'overflow_to_sample_mapping'],\n",
       "    num_rows: 4299\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect activations from GPT2\n",
    "sample = tokenized[0:2][\"input_ids\"]\n",
    "\n",
    "j, k = 1, 2\n",
    "lam = 1e-2\n",
    "(\n",
    "    activation_positions,\n",
    "    consequent_embeddings_intervened,\n",
    "    causal_embeddings,\n",
    "    v_j,\n",
    "    v_k,\n",
    ") = process_text(\n",
    "    model,\n",
    "    {\n",
    "        \"input_ids\": sample.cuda(),\n",
    "        \"attention_mask\": torch.ones_like(sample, device=\"cuda:0\"),\n",
    "    },\n",
    "    j,\n",
    "    k,\n",
    "    lam,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    1,\n",
    "    1,\n",
    ")\n",
    "(_, consequent_embeddings_clean, _, _, _) = process_text(\n",
    "    model,\n",
    "    {\n",
    "        \"input_ids\": sample.cuda(),\n",
    "        \"attention_mask\": torch.ones_like(sample, device=\"cuda:0\"),\n",
    "    },\n",
    "    j,\n",
    "    k,\n",
    "    0,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    1,\n",
    "    1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call, jacrev, vmap\n",
    "\n",
    "\n",
    "def compute_jacobian(model, j_activations, pos, j, k):\n",
    "    \"\"\"\n",
    "    Compute the batched Jacobians of layer k's activations with respect to layer j's activations for some select K tokens.\n",
    "\n",
    "    Args:\n",
    "    - model: GPT2Model instance\n",
    "    - j_activations: activations of layer j (shape: [batch_size, seq_len, hidden_size])\n",
    "    - pos: token positions of shape (B, K)\n",
    "    - j: index of the input layer\n",
    "    - k: index of the output layer\n",
    "\n",
    "    Returns:\n",
    "    - Batch of Jacobians\n",
    "    \"\"\"\n",
    "    # Ensure j_activations requires grad\n",
    "    j_activations.requires_grad_(True)\n",
    "\n",
    "    # Forward pass to get k_activations\n",
    "    def forward_to_k(x):\n",
    "        # Forward pass from j to k\n",
    "        activations = x.unsqueeze(1)\n",
    "        for layer_idx in range(j, k + 1):\n",
    "\n",
    "            def flayer(inputs):\n",
    "                if \"gpt\" in model_name:\n",
    "                    layer, params = (\n",
    "                        model.transformer.h[layer_idx],\n",
    "                        dict(model.transformer.h[layer_idx].named_parameters()),\n",
    "                    )\n",
    "                else:\n",
    "                    layer, params = (\n",
    "                        model.gpt_neox.layers[layer_idx],\n",
    "                        dict(model.gpt_neox.layers[layer_idx].named_parameters()),\n",
    "                    )\n",
    "\n",
    "                return functional_call(\n",
    "                    layer,\n",
    "                    params,\n",
    "                    inputs,\n",
    "                )[0]\n",
    "\n",
    "            activations = flayer(activations)\n",
    "\n",
    "        return activations\n",
    "\n",
    "    # good idea to sum here to reduce bias?\n",
    "    j_activations = j_activations.gather(\n",
    "        1, pos.unsqueeze(-1).expand(-1, -1, j_activations.shape[-1])\n",
    "    ).sum(dim=1, keepdim=True)\n",
    "\n",
    "    # Compute Jacobian\n",
    "    jacobian = vmap(jacrev(forward_to_k))(j_activations)\n",
    "\n",
    "    return jacobian.squeeze()\n",
    "    # But if we're pre-computing, we could just return the jacobian.squeeze(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1]) torch.Size([2, 64, 768])\n",
      "Jacobian shape: torch.Size([2, 768, 768])\n"
     ]
    }
   ],
   "source": [
    "# Generate random input\n",
    "# batch_size, seq_len = 1, 10\n",
    "# j_activations = torch.randn(batch_size, seq_len, 768, device=\"cuda:1\")\n",
    "i = torch.zeros(causal_embeddings.shape[0], 1, device=\"cuda\").long()\n",
    "\n",
    "print(i.shape, causal_embeddings.shape)\n",
    "\n",
    "jacobian = compute_jacobian(model, causal_embeddings, i, j, k)\n",
    "\n",
    "print(f\"Jacobian shape: {jacobian.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_a: 9.834899117322493e-08\n"
     ]
    }
   ],
   "source": [
    "# Check consequent_embeddings ~= original_embeddings_at_the_higher_layer + jacobian @ v_j * lam\n",
    "with torch.no_grad():\n",
    "    jacobian_approx = (\n",
    "        consequent_embeddings_clean\n",
    "        + torch.bmm(\n",
    "            v_j.gather(1, i.unsqueeze(-1).expand(-1, -1, v_j.shape[-1])),\n",
    "            jacobian.transpose(-2, -1),\n",
    "        )\n",
    "        * lam\n",
    "    )\n",
    "\n",
    "error = torch.mean((consequent_embeddings_intervened - jacobian_approx) ** 2)\n",
    "\n",
    "print(f\"error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_latents_first_pos(\n",
    "    output, feature_encoder_weights, feature_encoder_bias, i, j\n",
    "):\n",
    "    # concat hidden states for layer range\n",
    "    all_hidden_states = torch.cat(\n",
    "        [output.hidden_states[idx] for idx in range(i, j + 1)], dim=-1\n",
    "    )\n",
    "    feature_activation = (\n",
    "        einsum(all_hidden_states, feature_encoder_weights.T, \"b s e, e n -> b s n\")\n",
    "        - feature_encoder_bias\n",
    "    )\n",
    "    max_feature_activation, _ = torch.max(feature_activation, dim=-1)\n",
    "    # get first positions where maximal feature is activated\n",
    "    first_activation_positions = (\n",
    "        (max_feature_activation > 0).float().argmax(dim=1, keepdim=True)\n",
    "    )\n",
    "    expanded_pos = first_activation_positions.unsqueeze(-1).expand(\n",
    "        -1, -1, all_hidden_states.shape[-1]\n",
    "    )\n",
    "    token_activations = all_hidden_states.gather(1, expanded_pos)\n",
    "    num_fired = (token_activations > 0).sum(dim=-1)\n",
    "\n",
    "    return num_fired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, max_seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, lower and upper bounds for layers\n",
    "i, j = 0, 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 8192\n",
    "bsz = 128\n",
    "# for each sample compute activation positions and fired pre-act latents\n",
    "sample = tokenized.select(range(num_samples))\n",
    "\n",
    "# dist = []\n",
    "\n",
    "# for batch in tqdm(sample.iter(bsz), total=num_samples // bsz):\n",
    "#     with torch.no_grad():\n",
    "#         out = model(\n",
    "#             input_ids=batch[\"input_ids\"].cuda().unsqueeze(0), output_hidden_states=True\n",
    "#         )\n",
    "#     num_active = get_active_latents_first_pos(\n",
    "#         out, feature_encoder_weights, feature_encoder_bias, i, j\n",
    "#     )\n",
    "#     dist.extend(num_active.squeeze().cpu().tolist())\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "# # Create the histogram\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.title(\"Active Latents on Tokens activating Max Feature\")\n",
    "# _ = plt.hist(dist, bins=30, edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_latents_heatmap(\n",
    "    output, feature_encoder_weights, feature_encoder_bias, i, j\n",
    "):\n",
    "    # Concatenate hidden states for layer range\n",
    "    all_hidden_states = torch.cat(\n",
    "        [output.hidden_states[idx] for idx in range(i, j + 1)], dim=-1\n",
    "    )\n",
    "\n",
    "    # Calculate feature activation\n",
    "    feature_activation = (\n",
    "        torch.einsum(\"bse,en->bsn\", all_hidden_states, feature_encoder_weights.T)\n",
    "        - feature_encoder_bias\n",
    "    )\n",
    "\n",
    "    # Count number of active latents for each token\n",
    "    num_active_latents = (feature_activation > 0).sum(dim=-1)\n",
    "\n",
    "    return num_active_latents\n",
    "\n",
    "\n",
    "def visualize_heatmap_batch(heatmap_data, token_labels_batch, max_tokens_display=30):\n",
    "    batch_size, seq_length = heatmap_data.shape\n",
    "\n",
    "    # Create a figure with subplots for each batch item\n",
    "    fig, axes = plt.subplots(batch_size, 1, figsize=(20, 3 * batch_size), squeeze=False)\n",
    "    fig.suptitle(\"Active Latents Heatmap (Batch)\", fontsize=16)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        ax = axes[b, 0]\n",
    "\n",
    "        # Limit the number of tokens displayed\n",
    "        display_tokens = min(seq_length, max_tokens_display)\n",
    "        heatmap = heatmap_data[b, :display_tokens].unsqueeze(0).cpu().numpy()\n",
    "        token_labels = token_labels_batch[b][:display_tokens]\n",
    "\n",
    "        sns.heatmap(\n",
    "            heatmap,\n",
    "            cmap=\"YlOrRd\",\n",
    "            xticklabels=token_labels,\n",
    "            yticklabels=[\"\"],\n",
    "            ax=ax,\n",
    "            cbar=(b == batch_size - 1),\n",
    "        )  # Only show colorbar for the last subplot\n",
    "\n",
    "        ax.set_title(f\"Batch item {b}\")\n",
    "        ax.set_xlabel(\"Tokens\")\n",
    "\n",
    "        # Rotate and align x-axis labels for better readability\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "        # Add ellipsis if not all tokens are displayed\n",
    "        if display_tokens < seq_length:\n",
    "            ax.text(\n",
    "                display_tokens + 0.5,\n",
    "                0.5,\n",
    "                \"...\",\n",
    "                verticalalignment=\"center\",\n",
    "                horizontalalignment=\"left\",\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_latents_heatmap(i, j):\n",
    "    sample_input = tokenized.select(range(4))\n",
    "    out = model(input_ids=sample_input[\"input_ids\"].cuda(), output_hidden_states=True)\n",
    "\n",
    "    active_latents_tokenwise = get_active_latents_heatmap(\n",
    "        out, feature_encoder_weights, feature_encoder_bias, i, j\n",
    "    )\n",
    "    visualize_heatmap_batch(\n",
    "        active_latents_tokenwise,\n",
    "        [tokenizer.convert_ids_to_tokens(x) for x in sample_input[\"input_ids\"]],\n",
    "        j,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_causal_attribution_strength(\n",
    "    j,\n",
    "    k,\n",
    "    model,\n",
    "    inputs,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    lambda_value: float = 1.0,\n",
    "    feature_select_k: int = 1,\n",
    "    num_tokens: int = 1,\n",
    "):\n",
    "    (\n",
    "        first_activation_positions,\n",
    "        consequent_embeddings_intervened,\n",
    "        causal_embeddings,\n",
    "        v_j,\n",
    "        v_k,\n",
    "    ) = process_text(\n",
    "        model,\n",
    "        inputs,\n",
    "        intervention_index,\n",
    "        readout_index,\n",
    "        lambda_value,\n",
    "        feature_encoder_weights,\n",
    "        feature_encoder_bias,\n",
    "        feature_decoder_weights,\n",
    "        feature_select_k,\n",
    "        num_tokens,\n",
    "    )\n",
    "\n",
    "    expanded_pos = first_activation_positions.unsqueeze(-1).expand(\n",
    "        -1, -1, causal_embeddings.shape[-1]\n",
    "    )\n",
    "    v_j = v_j.gather(1, expanded_pos).squeeze(1)\n",
    "    v_k = v_k.gather(1, expanded_pos).squeeze(1)\n",
    "\n",
    "    jacobian = compute_jacobian(\n",
    "        model, causal_embeddings, first_activation_positions, j, k\n",
    "    )\n",
    "    # proportion of causality explained: compute vk.T(Jv_j) / ||v_k||^2\n",
    "    v_k_norm_squared = torch.sum(v_k**2, dim=-1)  # shape: (B,)\n",
    "    # Compute the whole expression using einsum\n",
    "    proportion_explained = (\n",
    "        torch.einsum(\"bie,be,bi->b\", jacobian, v_j, v_k) / v_k_norm_squared\n",
    "    )\n",
    "    # print(torch.einsum(\"bie,be,bi->b\", jacobian, v_j, v_k).shape, v_k_norm_squared.shape)\n",
    "    # error term\n",
    "    pred = torch.einsum(\"bie,be->bi\", jacobian, v_j)\n",
    "    strength = F.cosine_similarity(pred, v_k, dim=-1)\n",
    "    error = torch.mean((pred - v_k) ** 2, dim=-1)\n",
    "\n",
    "    return proportion_explained, strength, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: unbatch the jacobian\n",
    "i, j = 0, 1\n",
    "\n",
    "inputs = {\"input_ids\": sample.select(range(4))[\"input_ids\"].cuda()}\n",
    "\n",
    "explained_causality, strengths, error = compute_causal_attribution_strength(\n",
    "    i,\n",
    "    j,\n",
    "    model,\n",
    "    inputs,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.4091, 1.3020, 1.4088, 1.4095], device='cuda:0'),\n",
       " tensor([0.8815, 0.8748, 0.8815, 0.8816], device='cuda:0'),\n",
       " tensor([0.0002, 0.0002, 0.0002, 0.0002], device='cuda:0'))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_causality, strengths, error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
