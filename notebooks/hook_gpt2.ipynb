{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing consequences of interventions on gpt2, and how they match up against our SAE's\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from einops import einsum\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sae.data import chunk_and_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output embeddings: torch.Size([2, 768])\n",
      "shape of causal embeddings: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from regex import F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"cuda:1\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "sae_ckpt = load_file(\n",
    "    \"/home/sid/tensor-sae/checkpoints/layer-4-test/sae.safetensors\", device=\"cuda:1\"\n",
    ")\n",
    "\n",
    "feature_encoder_weights = sae_ckpt[\"encoder.weight\"]\n",
    "feature_encoder_bias = sae_ckpt[\"encoder.bias\"]\n",
    "# legacy keys\n",
    "feature_decoder_weights = sae_ckpt[\"W_dec\"]\n",
    "feature_decoder_bias = sae_ckpt[\"b_dec\"]\n",
    "\n",
    "intervention_index = 5\n",
    "readout_index = 8\n",
    "\n",
    "\n",
    "def create_hooks(\n",
    "    model,\n",
    "    intervention_index,\n",
    "    readout_index,\n",
    "    lambda_value,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "):\n",
    "    first_activation_positions = None\n",
    "    consequent_embeddings = None\n",
    "    causal_embeddings = None\n",
    "    v_j = None\n",
    "\n",
    "    def strengthen_sae_feature(module, input, output, layer_offset=0):\n",
    "        nonlocal first_activation_positions, causal_embeddings, v_j\n",
    "\n",
    "        embed_dim = output[0].shape[-1]\n",
    "        feature_encoder_segment = feature_encoder_weights[\n",
    "            :,\n",
    "            (intervention_index - layer_offset) * embed_dim : (\n",
    "                intervention_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "        feature_decoder_segment = feature_decoder_weights[\n",
    "            :,\n",
    "            (readout_index - layer_offset) * embed_dim : (\n",
    "                readout_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "\n",
    "        feature_activation = (\n",
    "            einsum(output[0], feature_encoder_segment.T, \"b s e, e n -> b s n\")\n",
    "            - feature_encoder_bias\n",
    "        )\n",
    "        feature_activation, max_feature_index = torch.max(feature_activation, dim=-1)\n",
    "\n",
    "        first_activation_positions = (feature_activation > 0).float().argmax(dim=1)\n",
    "        has_activation = (feature_activation > 0).any(dim=1)\n",
    "        first_activation_positions[~has_activation] = -1\n",
    "\n",
    "        batch_size, seq_len, embed_dim = output[0].shape\n",
    "\n",
    "        mask = torch.arange(seq_len, device=output[0].device).unsqueeze(0).expand(\n",
    "            batch_size, -1\n",
    "        ) == first_activation_positions.unsqueeze(1)\n",
    "\n",
    "        causal_embeddings = output[0]\n",
    "        mask = mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "\n",
    "        v_j = (\n",
    "            feature_decoder_segment.unsqueeze(0)\n",
    "            .expand(output[0].shape[0], -1, -1)\n",
    "            .gather(1, max_feature_index.unsqueeze(-1).expand(-1, -1, embed_dim))\n",
    "        )\n",
    "        new_output = output[0] + lambda_value * mask * v_j\n",
    "\n",
    "        new_outputs = [new_output] + list(output[1:])\n",
    "        return tuple(new_outputs)\n",
    "\n",
    "    def return_consequent_layer(module, input, output):\n",
    "        nonlocal consequent_embeddings, first_activation_positions\n",
    "\n",
    "        batch_size, seq_len, embed_dim = output[0].shape\n",
    "\n",
    "        mask = torch.arange(seq_len, device=output[0].device).unsqueeze(0).expand(\n",
    "            batch_size, -1\n",
    "        ) == first_activation_positions.unsqueeze(1)\n",
    "        mask = mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "\n",
    "        filtered_output = output[0]\n",
    "        consequent_embeddings = filtered_output.sum(dim=1)\n",
    "\n",
    "        # Return the original output unchanged\n",
    "        return output\n",
    "\n",
    "    intervention_hook = model.transformer.h[intervention_index].register_forward_hook(\n",
    "        partial(strengthen_sae_feature, layer_offset=intervention_index)\n",
    "    )\n",
    "    readout_hook = model.transformer.h[readout_index].register_forward_hook(\n",
    "        return_consequent_layer\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        intervention_hook,\n",
    "        readout_hook,\n",
    "        lambda: consequent_embeddings,\n",
    "        lambda: causal_embeddings,\n",
    "        lambda: v_j,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_text(\n",
    "    model,\n",
    "    inputs,\n",
    "    intervention_index,\n",
    "    readout_index,\n",
    "    lam,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "):\n",
    "    (\n",
    "        intervention_hook,\n",
    "        readout_hook,\n",
    "        get_consequent_embeddings,\n",
    "        get_causal_embeddings,\n",
    "        get_v_j,\n",
    "    ) = create_hooks(\n",
    "        model,\n",
    "        intervention_index,\n",
    "        readout_index,\n",
    "        lam,\n",
    "        feature_encoder_weights,\n",
    "        feature_encoder_bias,\n",
    "        feature_decoder_weights,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "    consequent_embeddings = get_consequent_embeddings()\n",
    "    causal_embeddings = get_causal_embeddings()\n",
    "    v_j = get_v_j()\n",
    "\n",
    "    intervention_hook.remove()\n",
    "    readout_hook.remove()\n",
    "\n",
    "    return (consequent_embeddings, causal_embeddings, v_j)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "intervention_index = 5\n",
    "readout_index = 8\n",
    "text = [\"Hello, world!\", \"Hello, world!\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "\n",
    "# Assuming you have these variables defined\n",
    "# feature_encoder_weights, feature_encoder_bias, feature_decoder_weights\n",
    "\n",
    "consequent_embeddings, causal_embeddings, _ = process_text(\n",
    "    model,\n",
    "    inputs,\n",
    "    intervention_index,\n",
    "    readout_index,\n",
    "    1.0,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    ")\n",
    "\n",
    "print(f\"shape of output embeddings: {consequent_embeddings.shape}\")\n",
    "print(f\"shape of causal embeddings: {causal_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next code we would need to add:\n",
    "#filter out any batch elements where the SAE doesn't trigger\n",
    "#compare to activations from a hook on the clean sequence\n",
    "#subtract, compute comparisons, etc.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I suspect the most efficient way to go about thos jacobian computation is to modify the gpt2 forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-11): 12 x GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GPT2SdpaAttention(\n",
       "      (c_attn): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jacobian(model, j_activations, i, j, k):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian of layer k's activations with respect to layer j's activations at position i.\n",
    "\n",
    "    Args:\n",
    "    - model: GPT2Model instance\n",
    "    - j_activations: activations of layer j (shape: [batch_size, seq_len, hidden_size])\n",
    "    - i: token position\n",
    "    - j: index of the input layer\n",
    "    - k: index of the output layer\n",
    "\n",
    "    Returns:\n",
    "    - Jacobian matrix\n",
    "    \"\"\"\n",
    "    # Ensure j_activations requires grad\n",
    "    j_activations.requires_grad_(True)\n",
    "\n",
    "    # Forward pass to get k_activations\n",
    "    def forward_to_k(x):\n",
    "        # Forward pass from j to k\n",
    "        activations = x\n",
    "        for layer_idx in range(j, k + 1):\n",
    "            activations = model.transformer.h[layer_idx](activations)[0]\n",
    "        return activations[:, i, :]\n",
    "\n",
    "    # Compute Jacobian\n",
    "    jacobian = torch.autograd.functional.jacobian(forward_to_k, j_activations)\n",
    "\n",
    "    return jacobian.squeeze(0, 2)[:, i, :]  # selecting only token pos i.\n",
    "    # But if we're pre-computing, we could just return the jacobian.squeeze(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 16. Reducing num_proc to 16 for dataset of size 16.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ").select(range(16))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, max_seq_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'overflow_to_sample_mapping'],\n",
       "    num_rows: 4299\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect activations from GPT2\n",
    "sample = tokenized[0][\"input_ids\"]\n",
    "j, k = 5, 8\n",
    "lam = 1e-9\n",
    "(consequent_embeddings, causal_embeddings, v_j) = process_text(\n",
    "    model,\n",
    "    {\n",
    "        \"input_ids\": sample.to(\"cuda:1\"),\n",
    "        \"attention_mask\": torch.ones_like(sample, device=\"cuda:1\"),\n",
    "    },\n",
    "    j,\n",
    "    k,\n",
    "    lam,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian shape: torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "# Generate random input\n",
    "# batch_size, seq_len = 1, 10\n",
    "# j_activations = torch.randn(batch_size, seq_len, 768, device=\"cuda:1\")\n",
    "\n",
    "# Compute Jacobian for predicting layer 5 activations from layer 3 activations at position 2\n",
    "i, j, k = 2, 3, 5\n",
    "jacobian = compute_jacobian(model, causal_embeddings, i, j, k)\n",
    "\n",
    "print(f\"Jacobian shape: {jacobian.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare jacobian and local approximation\n",
    "with torch.no_grad():\n",
    "    jacobian_approx = causal_embeddings[:, i, :] + jacobian * v_j[:, i, :] * lam\n",
    "    local_approx = causal_embeddings[:, i, :] + v_j[:, i, :] * lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:1')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(jacobian_approx - local_approx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
