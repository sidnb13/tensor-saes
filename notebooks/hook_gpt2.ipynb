{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing consequences of interventions on gpt2, and how they match up against our SAE's\n",
    "import torch\n",
    "\n",
    "feature_encoder_weights = torch.randn(768)\n",
    "feature_encoder_bias = torch.randn(1)\n",
    "\n",
    "feature_decoder_weights = torch.randn(768)\n",
    "feature_decoder_bias = torch.randn(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn( [2,4,5])\n",
    "b = torch.randn( [2, 5,1])\n",
    "(a @ b).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output embeddings: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Define the hook function\n",
    "def strengthen_sae_feature(module, input, output):\n",
    "    #identify first position where the SAE fires\n",
    "    # Compute the feature activation\n",
    "\n",
    "    #output: batch, seq, embed\n",
    "    #feature_encoder_weights: 1 , embed , 1  \n",
    "    # multiply result: batch, seq,\n",
    "    # #we want multiply result of batch, seq, embed\n",
    "    \n",
    "    #feature_activation = (output @ feature_encoder_weights.unsqueeze(0).unsqueeze(2)).squeeze(1) - feature_encoder_bias\n",
    "    #This way of doing it with einsum might be better\n",
    "    feature_activation = torch.einsum('bse,e->bs', output[0], feature_encoder_weights) - feature_encoder_bias\n",
    "\n",
    "    # Find the first position where the feature activates (if any)\n",
    "    first_activation_positions = (feature_activation > 0).float().argmax(dim=1)\n",
    "\n",
    "    # Check if the feature activates at all (argmax will return 0 if no activation)\n",
    "    has_activation = (feature_activation > 0).any(dim=1)\n",
    "\n",
    "    # Set positions to -1 where there's no activation\n",
    "    first_activation_positions[~has_activation] = -1\n",
    "\n",
    "    # Store the result\n",
    "    global first_activation_positions_global\n",
    "    first_activation_positions_global = first_activation_positions\n",
    "\n",
    "    global store_output\n",
    "    store_output = output\n",
    "\n",
    "    # Now, in only those positions, add lambda times feature_decoder_weights to the output\n",
    "    lambda_value = 1.0  # You can adjust this value as needed\n",
    "    batch_size, seq_len, embed_dim = output[0].shape\n",
    "    \n",
    "    # Create a mask for the positions where we want to add the feature\n",
    "    mask = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1) == first_activation_positions_global.unsqueeze(1)\n",
    "    mask = mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "    \n",
    "    # Add lambda times feature_decoder_weights to the output at the masked positions\n",
    "    new_output = output[0] + lambda_value * mask * feature_decoder_weights.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    new_outputs = [new_output] + list(output[1:])\n",
    "    return tuple(new_outputs)\n",
    "\n",
    "def return_consequent_layer(module, input, output):\n",
    "\n",
    "    # Get the batch size and sequence length from the output\n",
    "    batch_size, seq_len, embed_dim = output[0].shape\n",
    "    \n",
    "    # Create a mask for the positions where the feature first activates\n",
    "    mask = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1) == first_activation_positions_global.unsqueeze(1)\n",
    "    \n",
    "    # Expand the mask to match the embedding dimension\n",
    "    mask = mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "    \n",
    "    # Use the mask to zero out all positions except where the feature first activates\n",
    "    filtered_output = output[0] * mask\n",
    "    \n",
    "    # Sum along the sequence dimension to get one embedding per batch item\n",
    "    # This will effectively select the embedding at the first activation position\n",
    "    # for each item in the batch\n",
    "    selected_embeddings = filtered_output.sum(dim=1)\n",
    "    \n",
    "    # Store the selected embeddings\n",
    "    global consequent_embeddings\n",
    "    consequent_embeddings = selected_embeddings\n",
    "\n",
    "    # # Store the output of the middle layer\n",
    "    # global modified_layer\n",
    "    # modified_layer = output\n",
    "\n",
    "\n",
    "intervention_index = 5\n",
    "readout_index = 8\n",
    "\n",
    "# Register the hook on the chosen middle layer\n",
    "intervention_hook = model.transformer.h[intervention_index].register_forward_hook(strengthen_sae_feature)\n",
    "readout_hook = model.transformer.h[readout_index].register_forward_hook(return_consequent_layer)\n",
    "\n",
    "#then we have to read out the modified layer! will be stored under consequent_embeddings, size: batch , embed\n",
    "\n",
    "# Example usage\n",
    "text = [\"Hello, world!\", \"Hello, world!\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Now middle_layer_output contains the output of the middle layer\n",
    "print(f\"shape of output embeddings: {consequent_embeddings.shape}\")\n",
    "\n",
    "\n",
    "# Remove the hook after use\n",
    "intervention_hook.remove()\n",
    "readout_hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_activation_positions_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8405, -0.1911,  1.7396,  ..., -2.1171,  0.0240,  1.9667],\n",
       "        [ 0.8405, -0.1911,  1.7396,  ..., -2.1171,  0.0240,  1.9667]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consequent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next code we would need to add:\n",
    "#filter out any batch elements where the SAE doesn't trigger\n",
    "#compare to activations from a hook on the clean sequence\n",
    "#subtract, compute comparisons, etc.!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
