{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing consequences of interventions on gpt2, and how they match up against our SAE's\n",
    "from functools import partial\n",
    "\n",
    "import datasets\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum\n",
    "from safetensors.torch import load_file\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from sae.data import chunk_and_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "ckpt_path = \"/home/sid/tensor-sae/checkpoints/all-layer-test/sae.safetensors\"\n",
    "# ckpt_path = \"/home/sid/tensor-sae/checkpoints/pythia14m-all-layers-rp1t/pythia14m-all-layers-rp1t-sample_20240901_123737/layers.0_layers.1_layers.2_layers.3_layers.4_layers.5/sae-2298.safetensors\"\n",
    "# model_name = \"EleutherAI/pythia-14m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output embeddings: torch.Size([2, 768])\n",
      "shape of causal embeddings: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# to use jacrevd need eager implementation\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, attn_implementation=\"eager\"\n",
    ").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sae_ckpt = load_file(ckpt_path, device=\"cuda\")\n",
    "\n",
    "feature_encoder_weights = sae_ckpt.get(\"encoder.weight\", sae_ckpt.get(\"weight\"))\n",
    "feature_encoder_bias = sae_ckpt.get(\"encoder.bias\", sae_ckpt.get(\"bias\"))\n",
    "# legacy keys\n",
    "feature_decoder_weights = sae_ckpt[\"decoder.weight\"]\n",
    "feature_decoder_bias = sae_ckpt[\"decoder.bias\"]\n",
    "\n",
    "intervention_index = 2\n",
    "readout_index = 4\n",
    "\n",
    "\n",
    "def create_hooks(\n",
    "    model,\n",
    "    intervention_index,\n",
    "    readout_index,\n",
    "    lambda_value,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    feature_select_k=1,  # take top k-th feature\n",
    "    num_tokens=1,\n",
    "):\n",
    "    activation_positions = None\n",
    "    consequent_embeddings = None\n",
    "    causal_embeddings = None\n",
    "    # j < k in layer idx\n",
    "    v_j = None\n",
    "    v_k = None\n",
    "\n",
    "    def strengthen_sae_feature(module, input, output, layer_offset=0):\n",
    "        nonlocal activation_positions, causal_embeddings, v_j, v_k\n",
    "\n",
    "        embed_dim = output[0].shape[-1]\n",
    "        feature_encoder_segment = feature_encoder_weights[\n",
    "            :,\n",
    "            (intervention_index - layer_offset) * embed_dim : (\n",
    "                intervention_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "        feature_decoder_segment = feature_decoder_weights[\n",
    "            :,\n",
    "            (intervention_index - layer_offset) * embed_dim : (\n",
    "                intervention_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "\n",
    "        feature_activation = (\n",
    "            einsum(output[0], feature_encoder_segment.T, \"b s e, e n -> b s n\")\n",
    "            - feature_encoder_bias\n",
    "        )\n",
    "        # shape (batch_size, seq_len, 1)\n",
    "        feature_activation, max_feature_index = torch.kthvalue(\n",
    "            feature_activation,\n",
    "            k=feature_activation.shape[-1] - feature_select_k,\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        activation_positions = (\n",
    "            (feature_activation > 0).float().topk(k=num_tokens, dim=1)[1]\n",
    "        )\n",
    "        has_activation = (feature_activation > 0).any(dim=1)\n",
    "        activation_positions[~has_activation] = -1\n",
    "\n",
    "        batch_size, seq_len, embed_dim = output[0].shape\n",
    "\n",
    "        mask = (\n",
    "            torch.arange(seq_len, device=output[0].device)[None, :].expand(\n",
    "                batch_size, -1\n",
    "            )\n",
    "            == activation_positions\n",
    "        )\n",
    "\n",
    "        causal_embeddings = output[0]\n",
    "\n",
    "        # (batch_size, seq_len, embed_dim)\n",
    "        v_j = (\n",
    "            feature_decoder_segment.unsqueeze(0)\n",
    "            .expand(output[0].shape[0], -1, -1)\n",
    "            .gather(1, max_feature_index.unsqueeze(-1).expand(-1, -1, embed_dim))\n",
    "        )\n",
    "\n",
    "        new_output = output[0] + lambda_value * mask[:, :, None] * v_j\n",
    "\n",
    "        intervention_decoder_segment = feature_decoder_weights[\n",
    "            :,\n",
    "            (readout_index - layer_offset) * embed_dim : (\n",
    "                readout_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "        v_k = (\n",
    "            intervention_decoder_segment.unsqueeze(0)\n",
    "            .expand(output[0].shape[0], -1, -1)\n",
    "            .gather(1, max_feature_index.unsqueeze(-1).expand(-1, -1, embed_dim))\n",
    "        )\n",
    "\n",
    "        new_outputs = [new_output] + list(output[1:])\n",
    "        return tuple(new_outputs)\n",
    "\n",
    "    def return_consequent_layer(module, input, output):\n",
    "        nonlocal consequent_embeddings, activation_positions\n",
    "\n",
    "        filtered_output = output[0]\n",
    "        # TODO: best to sum over the tokens? One way of reducing bias\n",
    "        consequent_embeddings = filtered_output.sum(dim=1)\n",
    "\n",
    "        # Return the original output unchanged\n",
    "        return output\n",
    "\n",
    "    if \"gpt\" in model_name:\n",
    "        intervention_hook = model.transformer.h[\n",
    "            intervention_index\n",
    "        ].register_forward_hook(\n",
    "            partial(strengthen_sae_feature, layer_offset=intervention_index)\n",
    "        )\n",
    "        readout_hook = model.transformer.h[readout_index].register_forward_hook(\n",
    "            return_consequent_layer\n",
    "        )\n",
    "    else:\n",
    "        intervention_hook = model.gpt_neox.layers[\n",
    "            intervention_index\n",
    "        ].register_forward_hook(\n",
    "            partial(strengthen_sae_feature, layer_offset=intervention_index)\n",
    "        )\n",
    "        readout_hook = model.gpt_neox.layers[readout_index].register_forward_hook(\n",
    "            return_consequent_layer\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        intervention_hook,\n",
    "        readout_hook,\n",
    "        lambda: activation_positions,\n",
    "        lambda: consequent_embeddings,\n",
    "        lambda: causal_embeddings,\n",
    "        lambda: v_j,\n",
    "        lambda: v_k,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_text(\n",
    "    model,\n",
    "    inputs,\n",
    "    intervention_index,\n",
    "    readout_index,\n",
    "    lam,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    feature_select_k,\n",
    "    num_tokens,\n",
    "):\n",
    "    (\n",
    "        intervention_hook,\n",
    "        readout_hook,\n",
    "        get_first_activation_positions,\n",
    "        get_consequent_embeddings,\n",
    "        get_causal_embeddings,\n",
    "        get_v_j,\n",
    "        get_v_k,\n",
    "    ) = create_hooks(\n",
    "        model,\n",
    "        intervention_index,\n",
    "        readout_index,\n",
    "        lam,\n",
    "        feature_encoder_weights,\n",
    "        feature_encoder_bias,\n",
    "        feature_decoder_weights,\n",
    "        feature_select_k,\n",
    "        num_tokens,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "    first_activation_positions = get_first_activation_positions()\n",
    "    consequent_embeddings = get_consequent_embeddings()\n",
    "    causal_embeddings = get_causal_embeddings()\n",
    "    v_j = get_v_j()\n",
    "    v_k = get_v_k()\n",
    "\n",
    "    intervention_hook.remove()\n",
    "    readout_hook.remove()\n",
    "\n",
    "    return (\n",
    "        first_activation_positions,\n",
    "        consequent_embeddings,\n",
    "        causal_embeddings,\n",
    "        v_j,\n",
    "        v_k,\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "intervention_index = 4\n",
    "readout_index = 5\n",
    "text = [\"Hello, world!\", \"Hello, world!\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Assuming you have these variables defined\n",
    "# feature_encoder_weights, feature_encoder_bias, feature_decoder_weights\n",
    "\n",
    "first_activation_positions, consequent_embeddings, causal_embeddings, _, _ = (\n",
    "    process_text(\n",
    "        model,\n",
    "        inputs,\n",
    "        intervention_index,\n",
    "        readout_index,\n",
    "        1.0,\n",
    "        feature_encoder_weights,\n",
    "        feature_encoder_bias,\n",
    "        feature_decoder_weights,\n",
    "        1,\n",
    "        1,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"shape of output embeddings: {consequent_embeddings.shape}\")\n",
    "print(f\"shape of causal embeddings: {causal_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next code we would need to add:\n",
    "#filter out any batch elements where the SAE doesn't trigger\n",
    "#compare to activations from a hook on the clean sequence\n",
    "#subtract, compute comparisons, etc.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I suspect the most efficient way to go about thos jacobian computation is to modify the gpt2 forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 16. Reducing num_proc to 16 for dataset of size 16.\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ").select(range(16))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, max_seq_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'overflow_to_sample_mapping'],\n",
       "    num_rows: 4299\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect activations from GPT2\n",
    "sample = tokenized[0:2][\"input_ids\"]\n",
    "\n",
    "j, k = 1, 2\n",
    "lam = 1e-2\n",
    "(\n",
    "    activation_positions,\n",
    "    consequent_embeddings_intervened,\n",
    "    causal_embeddings,\n",
    "    v_j,\n",
    "    v_k,\n",
    ") = process_text(\n",
    "    model,\n",
    "    {\n",
    "        \"input_ids\": sample.cuda(),\n",
    "        \"attention_mask\": torch.ones_like(sample, device=\"cuda:0\"),\n",
    "    },\n",
    "    j,\n",
    "    k,\n",
    "    lam,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    1,\n",
    "    1,\n",
    ")\n",
    "(_, consequent_embeddings_clean, _, _, _) = process_text(\n",
    "    model,\n",
    "    {\n",
    "        \"input_ids\": sample.cuda(),\n",
    "        \"attention_mask\": torch.ones_like(sample, device=\"cuda:0\"),\n",
    "    },\n",
    "    j,\n",
    "    k,\n",
    "    0,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    1,\n",
    "    1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call, jacrev, vmap\n",
    "\n",
    "\n",
    "def compute_jacobian(model, j_activations, pos, j, k):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian of layer k's activations with respect to layer j's activations for some select K tokens.\n",
    "\n",
    "    Args:\n",
    "    - model: GPT2Model instance\n",
    "    - j_activations: activations of layer j (shape: [batch_size, seq_len, hidden_size])\n",
    "    - pos: token positions of shape (B, K)\n",
    "    - j: index of the input layer\n",
    "    - k: index of the output layer\n",
    "\n",
    "    Returns:\n",
    "    - Jacobian matrix\n",
    "    \"\"\"\n",
    "    # Ensure j_activations requires grad\n",
    "    j_activations.requires_grad_(True)\n",
    "\n",
    "    # Forward pass to get k_activations\n",
    "    def forward_to_k(x):\n",
    "        # Forward pass from j to k\n",
    "        activations = x.unsqueeze(1)\n",
    "        for layer_idx in range(j, k + 1):\n",
    "\n",
    "            def flayer(inputs):\n",
    "                if \"gpt\" in model_name:\n",
    "                    layer, params = (\n",
    "                        model.transformer.h[layer_idx],\n",
    "                        dict(model.transformer.h[layer_idx].named_parameters()),\n",
    "                    )\n",
    "                else:\n",
    "                    layer, params = (\n",
    "                        model.gpt_neox.layers[layer_idx],\n",
    "                        dict(model.gpt_neox.layers[layer_idx].named_parameters()),\n",
    "                    )\n",
    "\n",
    "                return functional_call(\n",
    "                    layer,\n",
    "                    params,\n",
    "                    inputs,\n",
    "                )[0]\n",
    "\n",
    "            activations = flayer(activations)\n",
    "\n",
    "        return activations\n",
    "\n",
    "    # TODO: good idea to sum here to reduce bias?\n",
    "    j_activations = j_activations.gather(\n",
    "        1, pos.unsqueeze(-1).expand(-1, -1, j_activations.shape[-1])\n",
    "    ).sum(dim=1, keepdim=True)\n",
    "\n",
    "    # Compute Jacobian\n",
    "    # jacobian = torch.autograd.functional.jacobian(forward_to_k, j_activations)\n",
    "    jacobian = vmap(jacrev(forward_to_k))(j_activations)\n",
    "\n",
    "    return jacobian.squeeze()\n",
    "    # But if we're pre-computing, we could just return the jacobian.squeeze(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1]) torch.Size([2, 64, 768])\n",
      "Jacobian shape: torch.Size([2, 768, 768])\n"
     ]
    }
   ],
   "source": [
    "# Generate random input\n",
    "# batch_size, seq_len = 1, 10\n",
    "# j_activations = torch.randn(batch_size, seq_len, 768, device=\"cuda:1\")\n",
    "i = torch.zeros(causal_embeddings.shape[0], 1, device=\"cuda\").long()\n",
    "\n",
    "print(i.shape, causal_embeddings.shape)\n",
    "\n",
    "jacobian = compute_jacobian(model, causal_embeddings, i, j, k)\n",
    "\n",
    "print(f\"Jacobian shape: {jacobian.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_a: 4.567054645576718e-08\n"
     ]
    }
   ],
   "source": [
    "# Check consequent_embeddings ~= original_embeddings_at_the_higher_layer + jacobian @ v_j * lam\n",
    "with torch.no_grad():\n",
    "    jacobian_approx_a = (\n",
    "        consequent_embeddings_clean + (v_j[:, i, :] @ jacobian.T) * lam\n",
    "    )\n",
    "\n",
    "error_a = torch.mean((consequent_embeddings_intervened - jacobian_approx_a) ** 2)\n",
    "\n",
    "print(f\"error_a: {error_a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_latents_first_pos(\n",
    "    output, feature_encoder_weights, feature_encoder_bias, i, j\n",
    "):\n",
    "    # concat hidden states for layer range\n",
    "    all_hidden_states = torch.cat(\n",
    "        [output.hidden_states[idx] for idx in range(i, j + 1)], dim=-1\n",
    "    )\n",
    "    feature_activation = (\n",
    "        einsum(all_hidden_states, feature_encoder_weights.T, \"b s e, e n -> b s n\")\n",
    "        - feature_encoder_bias\n",
    "    )\n",
    "    max_feature_activation, _ = torch.max(feature_activation, dim=-1)\n",
    "    # get first positions where maximal feature is activated\n",
    "    first_activation_positions = (\n",
    "        (max_feature_activation > 0).float().argmax(dim=1, keepdim=True)\n",
    "    )\n",
    "    expanded_pos = first_activation_positions.unsqueeze(-1).expand(\n",
    "        -1, -1, all_hidden_states.shape[-1]\n",
    "    )\n",
    "    token_activations = all_hidden_states.gather(1, expanded_pos)\n",
    "    num_fired = (token_activations > 0).sum(dim=-1)\n",
    "\n",
    "    return num_fired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, max_seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, lower and upper bounds for layers\n",
    "i, j = 0, 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 8192\n",
    "bsz = 128\n",
    "# for each sample compute activation positions and fired pre-act latents\n",
    "sample = tokenized.select(range(num_samples))\n",
    "\n",
    "# dist = []\n",
    "\n",
    "# for batch in tqdm(sample.iter(bsz), total=num_samples // bsz):\n",
    "#     with torch.no_grad():\n",
    "#         out = model(\n",
    "#             input_ids=batch[\"input_ids\"].cuda().unsqueeze(0), output_hidden_states=True\n",
    "#         )\n",
    "#     num_active = get_active_latents_first_pos(\n",
    "#         out, feature_encoder_weights, feature_encoder_bias, i, j\n",
    "#     )\n",
    "#     dist.extend(num_active.squeeze().cpu().tolist())\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "# # Create the histogram\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.title(\"Active Latents on Tokens activating Max Feature\")\n",
    "# _ = plt.hist(dist, bins=30, edgecolor=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_latents_heatmap(\n",
    "    output, feature_encoder_weights, feature_encoder_bias, i, j\n",
    "):\n",
    "    # Concatenate hidden states for layer range\n",
    "    all_hidden_states = torch.cat(\n",
    "        [output.hidden_states[idx] for idx in range(i, j + 1)], dim=-1\n",
    "    )\n",
    "\n",
    "    # Calculate feature activation\n",
    "    feature_activation = (\n",
    "        torch.einsum(\"bse,en->bsn\", all_hidden_states, feature_encoder_weights.T)\n",
    "        - feature_encoder_bias\n",
    "    )\n",
    "\n",
    "    # Count number of active latents for each token\n",
    "    num_active_latents = (feature_activation > 0).sum(dim=-1)\n",
    "\n",
    "    return num_active_latents\n",
    "\n",
    "\n",
    "def visualize_heatmap_batch(heatmap_data, token_labels_batch, max_tokens_display=30):\n",
    "    batch_size, seq_length = heatmap_data.shape\n",
    "\n",
    "    # Create a figure with subplots for each batch item\n",
    "    fig, axes = plt.subplots(batch_size, 1, figsize=(20, 3 * batch_size), squeeze=False)\n",
    "    fig.suptitle(\"Active Latents Heatmap (Batch)\", fontsize=16)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        ax = axes[b, 0]\n",
    "\n",
    "        # Limit the number of tokens displayed\n",
    "        display_tokens = min(seq_length, max_tokens_display)\n",
    "        heatmap = heatmap_data[b, :display_tokens].unsqueeze(0).cpu().numpy()\n",
    "        token_labels = token_labels_batch[b][:display_tokens]\n",
    "\n",
    "        sns.heatmap(\n",
    "            heatmap,\n",
    "            cmap=\"YlOrRd\",\n",
    "            xticklabels=token_labels,\n",
    "            yticklabels=[\"\"],\n",
    "            ax=ax,\n",
    "            cbar=(b == batch_size - 1),\n",
    "        )  # Only show colorbar for the last subplot\n",
    "\n",
    "        ax.set_title(f\"Batch item {b}\")\n",
    "        ax.set_xlabel(\"Tokens\")\n",
    "\n",
    "        # Rotate and align x-axis labels for better readability\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "        # Add ellipsis if not all tokens are displayed\n",
    "        if display_tokens < seq_length:\n",
    "            ax.text(\n",
    "                display_tokens + 0.5,\n",
    "                0.5,\n",
    "                \"...\",\n",
    "                verticalalignment=\"center\",\n",
    "                horizontalalignment=\"left\",\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_latents_heatmap(i, j):\n",
    "    sample_input = tokenized.select(range(4))\n",
    "    out = model(input_ids=sample_input[\"input_ids\"].cuda(), output_hidden_states=True)\n",
    "\n",
    "    active_latents_tokenwise = get_active_latents_heatmap(\n",
    "        out, feature_encoder_weights, feature_encoder_bias, i, j\n",
    "    )\n",
    "    visualize_heatmap_batch(\n",
    "        active_latents_tokenwise,\n",
    "        [tokenizer.convert_ids_to_tokens(x) for x in sample_input[\"input_ids\"]],\n",
    "        j,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_causal_attribution_strength(\n",
    "    j,\n",
    "    k,\n",
    "    model,\n",
    "    inputs,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    lambda_value: float = 1.0,\n",
    "    feature_select_k: int = 1,\n",
    "    num_tokens: int = 1,\n",
    "):\n",
    "    (\n",
    "        first_activation_positions,\n",
    "        consequent_embeddings_intervened,\n",
    "        causal_embeddings,\n",
    "        v_j,\n",
    "        v_k,\n",
    "    ) = process_text(\n",
    "        model,\n",
    "        inputs,\n",
    "        intervention_index,\n",
    "        readout_index,\n",
    "        lambda_value,\n",
    "        feature_encoder_weights,\n",
    "        feature_encoder_bias,\n",
    "        feature_decoder_weights,\n",
    "        feature_select_k,\n",
    "        num_tokens,\n",
    "    )\n",
    "\n",
    "    expanded_pos = first_activation_positions.unsqueeze(-1).expand(\n",
    "        -1, -1, causal_embeddings.shape[-1]\n",
    "    )\n",
    "    v_j = v_j.gather(1, expanded_pos).squeeze(1)\n",
    "    v_k = v_k.gather(1, expanded_pos).squeeze(1)\n",
    "\n",
    "    jacobian = torch.stack(\n",
    "        [\n",
    "            compute_jacobian(\n",
    "                model, causal_embeddings[idx].unsqueeze(0), pos.item(), j, k\n",
    "            )\n",
    "            for idx, pos in enumerate(first_activation_positions)\n",
    "        ]\n",
    "    )\n",
    "    # proportion of causality explained: compute vk.T(Jv_j) / ||v_k||^2\n",
    "    v_k_norm_squared = torch.sum(v_k**2, dim=-1)  # shape: (B,)\n",
    "    # Compute the whole expression using einsum\n",
    "    proportion_explained = (\n",
    "        torch.einsum(\"bie,be,bi->b\", jacobian, v_j, v_k) / v_k_norm_squared\n",
    "    )\n",
    "    # print(torch.einsum(\"bie,be,bi->b\", jacobian, v_j, v_k).shape, v_k_norm_squared.shape)\n",
    "    # error term\n",
    "    pred = torch.einsum(\"bie,be->bi\", jacobian, v_j)\n",
    "    strength = F.cosine_similarity(pred, v_k, dim=-1)\n",
    "    error = torch.mean((pred - v_k) ** 2, dim=-1)\n",
    "\n",
    "    return proportion_explained, strength, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m i, j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()}\n\u001b[0;32m----> 6\u001b[0m explained_causality, strengths, error \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_causal_attribution_strength\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_encoder_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_encoder_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_decoder_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 39\u001b[0m, in \u001b[0;36mcompute_causal_attribution_strength\u001b[0;34m(j, k, model, inputs, feature_encoder_weights, feature_encoder_bias, feature_decoder_weights, lambda_value, feature_select_k, num_tokens)\u001b[0m\n\u001b[1;32m     35\u001b[0m v_j \u001b[38;5;241m=\u001b[39m v_j\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, expanded_pos)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m v_k \u001b[38;5;241m=\u001b[39m v_k\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, expanded_pos)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m---> 39\u001b[0m     [\n\u001b[1;32m     40\u001b[0m         compute_jacobian(\n\u001b[1;32m     41\u001b[0m             model, causal_embeddings[idx]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), pos\u001b[38;5;241m.\u001b[39mitem(), j, k\n\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(first_activation_positions)\n\u001b[1;32m     44\u001b[0m     ]\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# proportion of causality explained: compute vk.T(Jv_j) / ||v_k||^2\u001b[39;00m\n\u001b[1;32m     47\u001b[0m v_k_norm_squared \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(v_k\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape: (B,)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m v_j \u001b[38;5;241m=\u001b[39m v_j\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, expanded_pos)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m v_k \u001b[38;5;241m=\u001b[39m v_k\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, expanded_pos)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m     39\u001b[0m     [\n\u001b[0;32m---> 40\u001b[0m         \u001b[43mcompute_jacobian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(first_activation_positions)\n\u001b[1;32m     44\u001b[0m     ]\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# proportion of causality explained: compute vk.T(Jv_j) / ||v_k||^2\u001b[39;00m\n\u001b[1;32m     47\u001b[0m v_k_norm_squared \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(v_k\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape: (B,)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 32\u001b[0m, in \u001b[0;36mcompute_jacobian\u001b[0;34m(model, j_activations, pos, j, k)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m activations\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# TODO: good idea to sum here to reduce bias?\u001b[39;00m\n\u001b[1;32m     31\u001b[0m j_activations \u001b[38;5;241m=\u001b[39m j_activations\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;241m1\u001b[39m, \u001b[43mpos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, j_activations\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     33\u001b[0m )\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Compute Jacobian\u001b[39;00m\n\u001b[1;32m     35\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mjacobian(forward_to_k, j_activations)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "# TODO: unbatch the jacobian\n",
    "i, j = 0, 1\n",
    "\n",
    "inputs = {\"input_ids\": sample.select(range(4))[\"input_ids\"].cuda()}\n",
    "\n",
    "explained_causality, strengths, error = compute_causal_attribution_strength(\n",
    "    i,\n",
    "    j,\n",
    "    model,\n",
    "    inputs,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "    1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.3011, 1.4086, 1.3021, 1.3011], device='cuda:0'),\n",
       " tensor([0.8747, 0.8816, 0.8749, 0.8747], device='cuda:0'),\n",
       " tensor([0.0002, 0.0002, 0.0002, 0.0002], device='cuda:0'))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_causality, strengths, error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
