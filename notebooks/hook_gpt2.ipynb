{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing consequences of interventions on gpt2, and how they match up against our SAE's\n",
    "import torch\n",
    "\n",
    "feature_encoder_weights = torch.randn(768)\n",
    "feature_encoder_bias = torch.randn(1)\n",
    "\n",
    "feature_decoder_weights = torch.randn(768)\n",
    "feature_decoder_bias = torch.randn(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn( [2,4,5])\n",
    "b = torch.randn( [2, 5,1])\n",
    "(a @ b).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output embeddings: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "# Define the hook function\n",
    "def strengthen_sae_feature(module, input, output):\n",
    "    # identify first position where the SAE fires\n",
    "    # Compute the feature activation\n",
    "\n",
    "    # output: batch, seq, embed\n",
    "    # feature_encoder_weights: 1 , embed , 1\n",
    "    # multiply result: batch, seq,\n",
    "    # #we want multiply result of batch, seq, embed\n",
    "\n",
    "    # feature_activation = (output @ feature_encoder_weights.unsqueeze(0).unsqueeze(2)).squeeze(1) - feature_encoder_bias\n",
    "    # This way of doing it with einsum might be better\n",
    "    feature_activation = (\n",
    "        torch.einsum(\"bse,e->bs\", output[0], feature_encoder_weights)\n",
    "        - feature_encoder_bias\n",
    "    )\n",
    "\n",
    "    # Find the first position where the feature activates (if any)\n",
    "    first_activation_positions = (feature_activation > 0).float().argmax(dim=1)\n",
    "\n",
    "    # Check if the feature activates at all (argmax will return 0 if no activation)\n",
    "    has_activation = (feature_activation > 0).any(dim=1)\n",
    "\n",
    "    # Set positions to -1 where there's no activation\n",
    "    first_activation_positions[~has_activation] = -1\n",
    "\n",
    "    # Store the result\n",
    "    global first_activation_positions_global\n",
    "    first_activation_positions_global = first_activation_positions\n",
    "\n",
    "    global store_output\n",
    "    store_output = output\n",
    "\n",
    "    # Now, in only those positions, add lambda times feature_decoder_weights to the output\n",
    "    lambda_value = 1.0  # You can adjust this value as needed\n",
    "    batch_size, seq_len, embed_dim = output[0].shape\n",
    "\n",
    "    # Create a mask for the positions where we want to add the feature\n",
    "    mask = torch.arange(seq_len).unsqueeze(0).expand(\n",
    "        batch_size, -1\n",
    "    ) == first_activation_positions_global.unsqueeze(1)\n",
    "    mask = mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "\n",
    "    # Add lambda times feature_decoder_weights to the output at the masked positions\n",
    "    new_output = output[0] + lambda_value * mask * feature_decoder_weights.unsqueeze(\n",
    "        0\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    new_outputs = [new_output] + list(output[1:])\n",
    "    return tuple(new_outputs)\n",
    "\n",
    "\n",
    "def return_consequent_layer(module, input, output):\n",
    "    # Get the batch size and sequence length from the output\n",
    "    batch_size, seq_len, embed_dim = output[0].shape\n",
    "\n",
    "    # Create a mask for the positions where the feature first activates\n",
    "    mask = torch.arange(seq_len).unsqueeze(0).expand(\n",
    "        batch_size, -1\n",
    "    ) == first_activation_positions_global.unsqueeze(1)\n",
    "\n",
    "    # Expand the mask to match the embedding dimension\n",
    "    mask = mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "\n",
    "    # Use the mask to zero out all positions except where the feature first activates\n",
    "    filtered_output = output[0] * mask\n",
    "\n",
    "    # Sum along the sequence dimension to get one embedding per batch item\n",
    "    # This will effectively select the embedding at the first activation position\n",
    "    # for each item in the batch\n",
    "    selected_embeddings = filtered_output.sum(dim=1)\n",
    "\n",
    "    # Store the selected embeddings\n",
    "    global consequent_embeddings\n",
    "    consequent_embeddings = selected_embeddings\n",
    "\n",
    "    # # Store the output of the middle layer\n",
    "    # global modified_layer\n",
    "    # modified_layer = output\n",
    "\n",
    "\n",
    "intervention_index = 5\n",
    "readout_index = 8\n",
    "\n",
    "# Register the hook on the chosen middle layer\n",
    "intervention_hook = model.transformer.h[intervention_index].register_forward_hook(\n",
    "    strengthen_sae_feature\n",
    ")\n",
    "readout_hook = model.transformer.h[readout_index].register_forward_hook(\n",
    "    return_consequent_layer\n",
    ")\n",
    "\n",
    "# then we have to read out the modified layer! will be stored under consequent_embeddings, size: batch , embed\n",
    "\n",
    "# Example usage\n",
    "text = [\"Hello, world!\", \"Hello, world!\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Now middle_layer_output contains the output of the middle layer\n",
    "print(f\"shape of output embeddings: {consequent_embeddings.shape}\")\n",
    "\n",
    "\n",
    "# Remove the hook after use\n",
    "intervention_hook.remove()\n",
    "readout_hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_activation_positions_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0194, -0.5493,  0.9223,  ..., -1.5387, -1.3595, -1.2544],\n",
       "        [ 0.0194, -0.5493,  0.9223,  ..., -1.5387, -1.3595, -1.2544]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consequent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next code we would need to add:\n",
    "#filter out any batch elements where the SAE doesn't trigger\n",
    "#compare to activations from a hook on the clean sequence\n",
    "#subtract, compute comparisons, etc.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I suspect the most efficient way to go about thos jacobian computation is to modify the gpt2 forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-11): 12 x GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GPT2SdpaAttention(\n",
       "      (c_attn): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jacobian(model, j_activations, i, j, k):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian of layer k's activations with respect to layer j's activations at position i.\n",
    "\n",
    "    Args:\n",
    "    - model: GPT2Model instance\n",
    "    - j_activations: activations of layer j (shape: [batch_size, seq_len, hidden_size])\n",
    "    - i: token position\n",
    "    - j: index of the input layer\n",
    "    - k: index of the output layer\n",
    "\n",
    "    Returns:\n",
    "    - Jacobian matrix\n",
    "    \"\"\"\n",
    "    # Ensure j_activations requires grad\n",
    "    j_activations.requires_grad_(True)\n",
    "\n",
    "    # Forward pass to get k_activations\n",
    "    def forward_to_k(x):\n",
    "        # Forward pass from j to k\n",
    "        activations = x\n",
    "        for layer_idx in range(j, k + 1):\n",
    "            activations = model.transformer.h[layer_idx](activations)[0]\n",
    "        return activations[:, i, :]\n",
    "\n",
    "    # Compute Jacobian\n",
    "    jacobian = torch.autograd.functional.jacobian(forward_to_k, j_activations)\n",
    "\n",
    "    return jacobian.squeeze(0, 2)[:, i, :]  # selecting only token pos i.\n",
    "    # But if we're pre-computing, we could just return the jacobian.squeeze(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian shape: torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate random input\n",
    "batch_size, seq_len = 1, 10\n",
    "j_activations = torch.randn(batch_size, seq_len, 768)\n",
    "\n",
    "# Compute Jacobian for predicting layer 5 activations from layer 3 activations at position 2\n",
    "i, j, k = 2, 3, 5\n",
    "jacobian = compute_jacobian(model, j_activations, i, j, k)\n",
    "\n",
    "print(f\"Jacobian shape: {jacobian.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
