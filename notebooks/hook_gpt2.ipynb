{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing consequences of interventions on gpt2, and how they match up against our SAE's\n",
    "import datasets\n",
    "import torch\n",
    "from einops import einsum\n",
    "from safetensors.torch import load_file\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sae.data import chunk_and_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output embeddings: torch.Size([2, 768])\n",
      "shape of causal embeddings: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").cuda()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "sae_ckpt = load_file(\n",
    "    \"/home/sid/tensor-sae/checkpoints/layer-4-test/sae.safetensors\", device=\"cuda\"\n",
    ")\n",
    "\n",
    "feature_encoder_weights = sae_ckpt[\"encoder.weight\"]\n",
    "feature_encoder_bias = sae_ckpt[\"encoder.bias\"]\n",
    "# legacy keys\n",
    "feature_decoder_weights = sae_ckpt[\"W_dec\"]\n",
    "feature_decoder_bias = sae_ckpt[\"b_dec\"]\n",
    "\n",
    "intervention_index = 5\n",
    "readout_index = 8\n",
    "\n",
    "\n",
    "def create_hooks(\n",
    "    model,\n",
    "    intervention_index,\n",
    "    readout_index,\n",
    "    lambda_value,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "):\n",
    "    first_activation_positions = None\n",
    "    consequent_embeddings = None\n",
    "    causal_embeddings = None\n",
    "    v_j = None\n",
    "\n",
    "    def strengthen_sae_feature(module, input, output, layer_offset=0):\n",
    "        nonlocal first_activation_positions, causal_embeddings, v_j\n",
    "\n",
    "        embed_dim = output[0].shape[-1]\n",
    "        feature_encoder_segment = feature_encoder_weights[\n",
    "            :,\n",
    "            (intervention_index - layer_offset) * embed_dim : (\n",
    "                intervention_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "        feature_decoder_segment = feature_decoder_weights[\n",
    "            :,\n",
    "            (readout_index - layer_offset) * embed_dim : (\n",
    "                readout_index - layer_offset + 1\n",
    "            )\n",
    "            * embed_dim,\n",
    "        ]\n",
    "\n",
    "        feature_activation = (\n",
    "            einsum(output[0], feature_encoder_segment.T, \"b s e, e n -> b s n\")\n",
    "            - feature_encoder_bias\n",
    "        )\n",
    "        feature_activation, max_feature_index = torch.max(feature_activation, dim=-1)\n",
    "\n",
    "        first_activation_positions = (feature_activation > 0).float().argmax(dim=1)\n",
    "        has_activation = (feature_activation > 0).any(dim=1)\n",
    "        first_activation_positions[~has_activation] = -1\n",
    "\n",
    "        batch_size, seq_len, embed_dim = output[0].shape\n",
    "\n",
    "        mask = torch.arange(seq_len, device=output[0].device).unsqueeze(0).expand(\n",
    "            batch_size, -1\n",
    "        ) == first_activation_positions.unsqueeze(1)\n",
    "\n",
    "        causal_embeddings = output[0]\n",
    "        mask = mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "\n",
    "        v_j = (\n",
    "            feature_decoder_segment.unsqueeze(0)\n",
    "            .expand(output[0].shape[0], -1, -1)\n",
    "            .gather(1, max_feature_index.unsqueeze(-1).expand(-1, -1, embed_dim))\n",
    "        )\n",
    "        new_output = output[0] + lambda_value * mask * v_j\n",
    "\n",
    "        new_outputs = [new_output] + list(output[1:])\n",
    "        return tuple(new_outputs)\n",
    "\n",
    "    def return_consequent_layer(module, input, output):\n",
    "        nonlocal consequent_embeddings, first_activation_positions\n",
    "\n",
    "        batch_size, seq_len, embed_dim = output[0].shape\n",
    "\n",
    "        mask = torch.arange(seq_len, device=output[0].device).unsqueeze(0).expand(\n",
    "            batch_size, -1\n",
    "        ) == first_activation_positions.unsqueeze(1)\n",
    "        mask = mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "\n",
    "        filtered_output = output[0]\n",
    "        consequent_embeddings = filtered_output.sum(dim=1)\n",
    "\n",
    "        # Return the original output unchanged\n",
    "        return output\n",
    "\n",
    "    intervention_hook = model.transformer.h[intervention_index].register_forward_hook(\n",
    "        partial(strengthen_sae_feature, layer_offset=intervention_index)\n",
    "    )\n",
    "    readout_hook = model.transformer.h[readout_index].register_forward_hook(\n",
    "        return_consequent_layer\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        intervention_hook,\n",
    "        readout_hook,\n",
    "        lambda: consequent_embeddings,\n",
    "        lambda: causal_embeddings,\n",
    "        lambda: v_j,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_text(\n",
    "    model,\n",
    "    inputs,\n",
    "    intervention_index,\n",
    "    readout_index,\n",
    "    lam,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    "):\n",
    "    (\n",
    "        intervention_hook,\n",
    "        readout_hook,\n",
    "        get_consequent_embeddings,\n",
    "        get_causal_embeddings,\n",
    "        get_v_j,\n",
    "    ) = create_hooks(\n",
    "        model,\n",
    "        intervention_index,\n",
    "        readout_index,\n",
    "        lam,\n",
    "        feature_encoder_weights,\n",
    "        feature_encoder_bias,\n",
    "        feature_decoder_weights,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "    consequent_embeddings = get_consequent_embeddings()\n",
    "    causal_embeddings = get_causal_embeddings()\n",
    "    v_j = get_v_j()\n",
    "\n",
    "    intervention_hook.remove()\n",
    "    readout_hook.remove()\n",
    "\n",
    "    return (consequent_embeddings, causal_embeddings, v_j)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "intervention_index = 5\n",
    "readout_index = 8\n",
    "text = [\"Hello, world!\", \"Hello, world!\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Assuming you have these variables defined\n",
    "# feature_encoder_weights, feature_encoder_bias, feature_decoder_weights\n",
    "\n",
    "consequent_embeddings, causal_embeddings, _ = process_text(\n",
    "    model,\n",
    "    inputs,\n",
    "    intervention_index,\n",
    "    readout_index,\n",
    "    1.0,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    ")\n",
    "\n",
    "print(f\"shape of output embeddings: {consequent_embeddings.shape}\")\n",
    "print(f\"shape of causal embeddings: {causal_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next code we would need to add:\n",
    "#filter out any batch elements where the SAE doesn't trigger\n",
    "#compare to activations from a hook on the clean sequence\n",
    "#subtract, compute comparisons, etc.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I suspect the most efficient way to go about thos jacobian computation is to modify the gpt2 forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-11): 12 x GPT2Block(\n",
       "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn): GPT2SdpaAttention(\n",
       "      (c_attn): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp): GPT2MLP(\n",
       "      (c_fc): Conv1D()\n",
       "      (c_proj): Conv1D()\n",
       "      (act): NewGELUActivation()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jacobian(model, j_activations, i, j, k):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian of layer k's activations with respect to layer j's activations at position i.\n",
    "\n",
    "    Args:\n",
    "    - model: GPT2Model instance\n",
    "    - j_activations: activations of layer j (shape: [batch_size, seq_len, hidden_size])\n",
    "    - i: token position\n",
    "    - j: index of the input layer\n",
    "    - k: index of the output layer\n",
    "\n",
    "    Returns:\n",
    "    - Jacobian matrix\n",
    "    \"\"\"\n",
    "    # Ensure j_activations requires grad\n",
    "    j_activations.requires_grad_(True)\n",
    "\n",
    "    # Forward pass to get k_activations\n",
    "    def forward_to_k(x):\n",
    "        # Forward pass from j to k\n",
    "        activations = x\n",
    "        for layer_idx in range(j, k + 1):\n",
    "            activations = model.transformer.h[layer_idx](activations)[0]\n",
    "        return activations[:, i, :]\n",
    "\n",
    "    # Compute Jacobian\n",
    "    jacobian = torch.autograd.functional.jacobian(forward_to_k, j_activations)\n",
    "\n",
    "    return jacobian.squeeze(0, 2)[:, i, :]  # selecting only token pos i.\n",
    "    # But if we're pre-computing, we could just return the jacobian.squeeze(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 16. Reducing num_proc to 16 for dataset of size 16.\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ").select(range(16))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, max_seq_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'overflow_to_sample_mapping'],\n",
       "    num_rows: 4299\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect activations from GPT2\n",
    "sample = tokenized[0][\"input_ids\"]\n",
    "j, k = 5, 8\n",
    "lam = 1e-9\n",
    "(consequent_embeddings, causal_embeddings, v_j) = process_text(\n",
    "    model,\n",
    "    {\n",
    "        \"input_ids\": sample.cuda(),\n",
    "        \"attention_mask\": torch.ones_like(sample, device=\"cuda:1\"),\n",
    "    },\n",
    "    j,\n",
    "    k,\n",
    "    lam,\n",
    "    feature_encoder_weights,\n",
    "    feature_encoder_bias,\n",
    "    feature_decoder_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian shape: torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "# Generate random input\n",
    "# batch_size, seq_len = 1, 10\n",
    "# j_activations = torch.randn(batch_size, seq_len, 768, device=\"cuda:1\")\n",
    "\n",
    "# Compute Jacobian for predicting layer 5 activations from layer 3 activations at position 2\n",
    "i, j, k = 2, 3, 5\n",
    "jacobian = compute_jacobian(model, causal_embeddings, i, j, k)\n",
    "\n",
    "print(f\"Jacobian shape: {jacobian.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare jacobian and local approximation\n",
    "with torch.no_grad():\n",
    "    jacobian_approx = causal_embeddings[:, i, :] + jacobian * v_j[:, i, :] * lam\n",
    "    local_approx = causal_embeddings[:, i, :] + v_j[:, i, :] * lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(jacobian_approx - local_approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_latents_first_pos(\n",
    "    output, feature_encoder_weights, feature_encoder_bias, i, j\n",
    "):\n",
    "    # concat hidden states for layer range\n",
    "    all_hidden_states = torch.cat(\n",
    "        [output.hidden_states[idx] for idx in range(i, j + 1)], dim=-1\n",
    "    )\n",
    "    feature_activation = (\n",
    "        einsum(all_hidden_states, feature_encoder_weights.T, \"b s e, e n -> b s n\")\n",
    "        - feature_encoder_bias\n",
    "    )\n",
    "    max_feature_activation, _ = torch.max(feature_activation, dim=-1)\n",
    "    first_activation_positions = (\n",
    "        (max_feature_activation > 0).float().argmax(dim=1, keepdim=True)\n",
    "    )\n",
    "    expanded_pos = first_activation_positions.unsqueeze(-1).expand(\n",
    "        -1, -1, all_hidden_states.shape[-1]\n",
    "    )\n",
    "    token_activations = all_hidden_states.gather(1, expanded_pos)\n",
    "    num_fired = (token_activations > 0).sum(dim=-1)\n",
    "\n",
    "    return num_fired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenized = chunk_and_tokenize(dataset, tokenizer, max_seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [01:39<00:00,  1.56s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "num_samples = 8192\n",
    "bsz = 256\n",
    "# for each sample compute activation positions and fired pre-act latents\n",
    "sample = tokenized.select(range(num_samples))\n",
    "\n",
    "dist = []\n",
    "\n",
    "for batch in tqdm(sample.iter(bsz), total=num_samples // bsz):\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            input_ids=batch[\"input_ids\"].cuda().unsqueeze(0), output_hidden_states=True\n",
    "        )\n",
    "    num_active = get_active_latents_first_pos(\n",
    "        out, feature_encoder_weights, feature_encoder_bias, 6, 11\n",
    "    )\n",
    "    dist.extend(num_active.squeeze().cpu().tolist())\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "_ = plt.hist(dist, bins=30, edgecolor='black')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
