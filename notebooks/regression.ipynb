{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from sae.data import chunk_and_tokenize\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x152c2e68bdf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba8ac6ada4c46548bf2c76df4b7e769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=48):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-1T-Sample\", split=\"train[:1000]\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "dataset = chunk_and_tokenize(dataset, tokenizer, max_seq_len=512)\n",
    "model = AutoModel.from_pretrained(\"gpt2\", device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.linear = nn.Linear(d_in, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NuclearNormLoss(nn.Module):\n",
    "    def __init__(self, lam=0.01, rank=None):\n",
    "        super().__init__()\n",
    "        self.lam = lam\n",
    "        self.rank = rank or 1\n",
    "\n",
    "    def forward(self, pred, target, weight):\n",
    "        mse_loss = torch.mean((pred - target) ** 2)\n",
    "        _, S, _ = torch.linalg.svd(weight)\n",
    "        # penalize all other ranks\n",
    "        nuclear_norm = torch.sum(S[self.rank :])\n",
    "        return mse_loss + self.lam * nuclear_norm, mse_loss, nuclear_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FVU: 3.8716983795166016\n",
      "FVU: 3.924262523651123\n",
      "FVU: 3.8814103603363037\n",
      "FVU: 3.8961524963378906\n",
      "FVU: 3.932107925415039\n",
      "FVU: 3.9003334045410156\n",
      "FVU: 3.884122371673584\n",
      "FVU: 3.7931056022644043\n",
      "FVU: 3.8936281204223633\n",
      "FVU: 3.913456439971924\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(\n",
    "    model,\n",
    "    dataset,\n",
    "    i,\n",
    "    j,\n",
    "    rank,\n",
    "    batch_size=64,\n",
    "    lam=0.1,\n",
    "    lr=1e-2,\n",
    "    steps=20,\n",
    "    eval_steps=10,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "    train_dataloader = DataLoader(\n",
    "        split_dataset[\"train\"], batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        split_dataset[\"test\"], batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    linear_model = Regression(d_in=model.config.n_embd, d_out=model.config.n_embd).to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    nn.init.zeros_(linear_model.linear.weight)\n",
    "    nn.init.zeros_(linear_model.linear.bias)\n",
    "\n",
    "    criterion = NuclearNormLoss(lam=lam, rank=rank)\n",
    "    opt = torch.optim.SGD(linear_model.parameters(), lr=lr)\n",
    "\n",
    "    train_iter = cycle(train_dataloader)\n",
    "\n",
    "    for step in range(steps):\n",
    "        train_batch = next(train_iter)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_out = model(\n",
    "                train_batch[\"input_ids\"].to(device), output_hidden_states=True\n",
    "            )\n",
    "        layer_i_acts, layer_j_acts = (\n",
    "            model_out.hidden_states[i],\n",
    "            model_out.hidden_states[j],\n",
    "        )\n",
    "\n",
    "        pred, target = (\n",
    "            linear_model(F.normalize(layer_i_acts, p=2, dim=-1)),\n",
    "            F.normalize(layer_j_acts, p=2, dim=-1),\n",
    "        )\n",
    "        loss, l2, nnorm = criterion(pred, target, linear_model.linear.weight)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            l2_unreduced = torch.mean((pred - target) ** 2, dim=0)\n",
    "            var = torch.var(target, dim=0)\n",
    "            # print(l2_unreduced.mean(-1), var.mean(-1))\n",
    "            fvu = (l2_unreduced / var).mean()\n",
    "\n",
    "            print(f\"FVU: {fvu}\")\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    linear_model.eval()\n",
    "    total_fvu = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, test_batch in enumerate(test_dataloader):\n",
    "            model_out = model(\n",
    "                test_batch[\"input_ids\"].to(device), output_hidden_states=True\n",
    "            )\n",
    "            layer_i_acts, layer_j_acts = (\n",
    "                model_out.hidden_states[i],\n",
    "                model_out.hidden_states[j],\n",
    "            )\n",
    "            pred, target = (\n",
    "                F.normalize(\n",
    "                    linear_model(F.normalize(layer_i_acts, p=2, dim=-1)), p=2, dim=-1\n",
    "                ),\n",
    "                F.normalize(layer_j_acts, p=2, dim=-1),\n",
    "            )\n",
    "            l2_unreduced = torch.mean((pred - target) ** 2, dim=0)\n",
    "            var = torch.var(target, dim=0)\n",
    "            fvu = (l2_unreduced / var).mean()\n",
    "\n",
    "            total_fvu += fvu.item()\n",
    "\n",
    "            if i >= eval_steps:\n",
    "                break\n",
    "\n",
    "    test_fvu = total_fvu / eval_steps\n",
    "    test_r2 = 1 - test_fvu\n",
    "\n",
    "    return test_r2\n",
    "\n",
    "\n",
    "def create_scree_plot(model, dataset, layer_pairs, ranks):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]  # Add more colors if needed\n",
    "\n",
    "    for idx, (i, j) in enumerate(layer_pairs):\n",
    "        r2_values = []\n",
    "        for rank in ranks:\n",
    "            r2 = train_and_evaluate(model, dataset, i, j, rank, steps=10)\n",
    "            r2_values.append(r2)\n",
    "\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.plot(ranks, r2_values, f\"{color}o-\", label=f\"Layers {i} to {j}\")\n",
    "\n",
    "    plt.xlabel(\"Rank\")\n",
    "    plt.ylabel(\"R^2\")\n",
    "    plt.title(\"Scree Plot for Multiple Layer Pairs\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage\n",
    "ranks = [1, 2, 4, 8, 16, 32, 384, 768]\n",
    "layer_pairs = [(1, 1), (1, 4), (1, 8)]  # Add more layer pairs as needed\n",
    "create_scree_plot(model, dataset, layer_pairs, ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
